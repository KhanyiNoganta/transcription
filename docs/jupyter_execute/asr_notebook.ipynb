{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c0149c",
   "metadata": {
    "id": "yzjZqUC1f3Qq"
   },
   "source": [
    "# ASR Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e19fe3",
   "metadata": {
    "id": "OI6WYuLoiXrJ"
   },
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6576390",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00q4ZFQriJbP",
    "outputId": "62a25360-1fab-41bf-cc54-b8456fec3e3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples 3\n",
      "train\n",
      "validation\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Afrispeech dataset\n",
    "dataset = load_dataset(\"tobiolatunji/afrispeech-200\", \"all\", streaming=True)\n",
    "print(\"Number of examples\", len(dataset))\n",
    "\n",
    "# Access the filtered examples\n",
    "for example in dataset:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760cc3e3",
   "metadata": {
    "id": "XDXjshxZjvnf"
   },
   "source": [
    "### Dropping some of the unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "418fbd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unused columns\n",
    "dataset_cln = dataset.remove_columns(['speaker_id', 'path', 'age_group', 'gender', 'accent', 'domain', 'country', 'duration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d1fd2",
   "metadata": {
    "id": "d4nt4RllokGM"
   },
   "source": [
    "### Prepare Feature Extractor, Tokenizer and Data\n",
    "\n",
    "- A feature extractor pre-processes the raw audio inputs\n",
    "- The model performs the sequence-to-sequence mapping\n",
    "- A tokenizer post-processes the model outputs to text format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5a51f2",
   "metadata": {
    "id": "naH3zHqypmeg"
   },
   "source": [
    "load the feature extractor from the pre-trained checkpoint with the default values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db90f179",
   "metadata": {
    "id": "Qu1GUCMjoeup"
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63cfd7e",
   "metadata": {
    "id": "P69rDmUmpuPd"
   },
   "source": [
    "#### load WhisperTokenizer\n",
    "\n",
    "The Whisper model outputs a sequence of token ids. The tokenizer maps each of these token ids to their corresponding text string. For the English language, we can load the pre-trained tokenizer and use it for fine-tuning without any further modifications. We simply have to specify the target language and the task. These arguments inform the tokenizer to prefix the language and task tokens to the start of encoded label sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cecde85a",
   "metadata": {
    "id": "1V9Nmo0xpfFM"
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"English\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f74853",
   "metadata": {
    "id": "qHlO3m8LqF3O"
   },
   "source": [
    "#### Combine To create A WhisperProcessor\n",
    "\n",
    "To simplify using the feature extractor and tokenizer, we can wrap both into a single WhisperProcessor class. This processor object inherits from WhisperFeatureExtractor and WhisperProcessor, and can be used on the audio inputs and model predictions as required. In doing so, we only need to keep track of two objects during training: the processor and the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c7dca77",
   "metadata": {
    "id": "EDtFjKxzqEHc"
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"English\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6566e6",
   "metadata": {
    "id": "PAb2Fr5QrJLy"
   },
   "source": [
    "### Data Preparation\n",
    "\n",
    "Since our input audio is sampled at 44100Hz, we need to downsample it to 16kHz prior to passing it to the Whisper feature extractor, 16kHz being the sampling rate expected by the Whisper model.\n",
    "\n",
    "We'll set the audio inputs to the correct sampling rate using dataset's cast_column method. This operation does not change the audio in-place, but rather signals to datasets to resample audio samples on the fly the first time that they are loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "661f94a1",
   "metadata": {
    "id": "ls_6eoXmzJWs"
   },
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "dataset_cln = dataset_cln.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dbc270",
   "metadata": {
    "id": "5ID8X1Gzzseh"
   },
   "source": [
    "## A function to prepare our data for the model:\n",
    "\n",
    "We first provide set constants for the maximum duration of audio in seconds, the maximum input length, and the maximum label length. The maximum input length and label length are important for defining the constraints and processing requirements for our audio data and its corresponding transcriptions within the model.\n",
    "\n",
    "We then load the resampled audio data by calling batch[\"audio\"].\n",
    "We use the feature extractor to compute the log-Mel spectrogram input features from our 1-dimensional audio array.\n",
    "We encode the transcriptions to label ids through the use of the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ef96f47",
   "metadata": {
    "id": "BlQKOSs3apxG"
   },
   "outputs": [],
   "source": [
    "# Pre-processing to suit the max audio length required by Whisper\n",
    "MAX_DURATION_IN_SECONDS = 30.0\n",
    "MAX_INPUT_LENGTH = MAX_DURATION_IN_SECONDS * 16000\n",
    "MAX_LABEL_LENGTH = 448\n",
    "\n",
    "# Add these functions to your code\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"transcript\"]).input_ids\n",
    "    batch[\"input_length\"] = len(batch[\"audio\"])\n",
    "    batch[\"labels_length\"] = len(tokenizer(batch[\"transcript\"], add_special_tokens=False).input_ids)\n",
    "    return batch\n",
    "def filter_inputs(input_length):\n",
    "    \"\"\"Filter inputs with zero input length or longer than 30s\"\"\"\n",
    "    return 0 < input_length < MAX_INPUT_LENGTH\n",
    "def filter_labels(labels_length):\n",
    "    \"\"\"Filter label sequences longer than max length (448)\"\"\"\n",
    "    return labels_length < MAX_LABEL_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8400ef67",
   "metadata": {
    "id": "u9QmkFvma4d_"
   },
   "outputs": [],
   "source": [
    "dataset_cln = dataset_cln.map(prepare_dataset)\n",
    "dataset_cln = dataset_cln.filter(filter_inputs, input_columns=[\"input_length\"])\n",
    "dataset_cln = dataset_cln.filter(filter_labels, input_columns=[\"labels_length\"])\n",
    "dataset_cln = dataset_cln.remove_columns(['labels_length', 'input_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd7453f1",
   "metadata": {
    "id": "kfKc27YLzdQn"
   },
   "outputs": [],
   "source": [
    "val_dataset = dataset_cln['train'].take(3000)\n",
    "train_dataset = dataset_cln['train'].skip(3000)\n",
    "\n",
    "dataset_cln['train'] = train_dataset\n",
    "dataset_cln['validation'] = val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fb9f8a1",
   "metadata": {
    "id": "YH9chxGVzdNe"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd2739f7",
   "metadata": {
    "id": "GgIWr5bvzdJj"
   },
   "outputs": [],
   "source": [
    "#  initialising the data collator defined above:\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25df7d77",
   "metadata": {
    "id": "gAz0yWibzdFn"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df5331d3",
   "metadata": {
    "id": "l3FcjuYdzdBz"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30b6ae89",
   "metadata": {
    "id": "w0z9a2dYzc-D"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548a8d899c314076a3a7e6a237eceeb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf28cc2",
   "metadata": {
    "id": "QVM5EKjOzc5A"
   },
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61eb3d1",
   "metadata": {
    "id": "83x1IeD1zcte"
   },
   "outputs": [],
   "source": [
    "# Training Args\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./dsn_afrispeech\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=4000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    "    resume_from_checkpoint=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba5e0b",
   "metadata": {
    "id": "mP88IkMGzbDb"
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset_cln['train'],\n",
    "    eval_dataset=dataset_cln['validation'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c244221",
   "metadata": {
    "id": "QnIcSltpzMNa"
   },
   "outputs": [],
   "source": [
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a15cab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 832
    },
    "id": "jb8-UERnI74A",
    "outputId": "6337fefe-39ec-4f43-ac58-32829db58986"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.15.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "source_map": [
   14,
   18,
   22,
   40,
   44,
   49,
   57,
   61,
   68,
   74,
   81,
   87,
   94,
   102,
   109,
   119,
   143,
   152,
   162,
   198,
   205,
   212,
   231,
   239,
   247,
   278,
   294,
   300
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}